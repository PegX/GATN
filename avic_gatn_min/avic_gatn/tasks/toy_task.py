from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Any, List, Optional, Tuple

import torch
import torch.nn as nn

from avic_gatn.models.gatn_min import GATNMinimal, Circuit


@dataclass
class MetricBundle:
    primary: float
    metrics: Dict[str, float]


class ToyGATNTaskAdapter:
    """MeVGT-style adapter for AViC@GATN minimal.

    We use a deterministic toy multi-label task:

    - `img_feat` is random but fixed given seed.
    - `node_feat` is trainable-like input (attack surface).
    - labels are generated by a fixed random linear teacher.

    Metric: average BCE (lower is better). We report `primary = -BCE` so higher is better.
    """

    def __init__(self, cfg: Dict[str, Any], device: torch.device):
        self.cfg = cfg
        self.device = device
        self.model: Optional[GATNMinimal] = None
        self._ablation: Optional[Circuit] = None

        self._teacher_w: Optional[torch.Tensor] = None
        self._rng = torch.Generator(device="cpu")

        self._last_cache: Dict[str, torch.Tensor] = {}

    def setup(self):
        mcfg = self.cfg["model"]
        self.model = GATNMinimal(
            num_classes=int(mcfg["num_classes"]),
            in_channel=int(mcfg["in_channel"]),
            hidden_gcn=int(mcfg["hidden_gcn"]),
            feat_dim=int(mcfg["feat_dim"]),
            num_heads=int(mcfg["num_heads"]),
        ).to(self.device)
        self.model.eval()

        # deterministic teacher to generate labels
        self._teacher_w = torch.randn(
            mcfg["feat_dim"], mcfg["num_classes"], generator=self._rng
        ).to(self.device)

    def teardown(self):
        pass

    def list_circuits(self) -> List[Circuit]:
        assert self.model is not None
        H = self.model.topo_block.attn1.num_heads
        return [Circuit(attn_id=a, head_id=h) for a in (0, 1) for h in range(H)]

    def set_ablation(self, circuit: Optional[Circuit]):
        assert self.model is not None
        self._ablation = circuit
        self.model.set_ablation(circuit)

    def _make_batch(self, batch_size: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        assert self.model is not None
        mcfg = self.cfg["model"]
        N = int(mcfg["num_classes"])
        Cin = int(mcfg["in_channel"])
        F = int(mcfg["feat_dim"])

        g = self._rng
        # fixed seed progression is ok; we want deterministic runs
        img_feat = torch.randn(batch_size, F, generator=g, device=self.device)
        node_feat = torch.randn(batch_size, N, Cin, generator=g, device=self.device)

        # topology priors: start from noisy identity + low-rank
        I = torch.eye(N, device=self.device).unsqueeze(0).repeat(batch_size, 1, 1)
        A1 = I + 0.05 * torch.randn(batch_size, N, N, generator=g, device=self.device)
        A2 = I + 0.05 * torch.randn(batch_size, N, N, generator=g, device=self.device)

        # teacher labels: sigmoid(img_feat @ W) with noise, threshold at 0.5
        logits_teacher = img_feat @ self._teacher_w  # [B,N]
        probs = torch.sigmoid(logits_teacher)
        y = (probs > 0.5).float()
        return img_feat, node_feat, A1, A2, y

    def forward_with_cache(self, img_feat, node_feat, A1, A2):
        assert self.model is not None
        logits, cache = self.model(img_feat, node_feat, A1, A2)
        self._last_cache = {k: (v.detach().clone()) for k, v in cache.items() if isinstance(v, torch.Tensor)}
        return logits

    def get_last_cache(self) -> Dict[str, torch.Tensor]:
        return dict(self._last_cache)

    @torch.no_grad()
    def evaluate(self, steps: int) -> MetricBundle:
        assert self.model is not None
        bsz = int(self.cfg["data"]["batch_size"])

        total_loss = 0.0
        total = 0

        for _ in range(steps):
            img_feat, node_feat, A1, A2, y = self._make_batch(bsz)
            logits = self.forward_with_cache(img_feat, node_feat, A1, A2)
            loss = nn.functional.binary_cross_entropy_with_logits(logits, y)
            total_loss += float(loss.item())
            total += 1

        avg_loss = total_loss / max(1, total)
        # primary: higher is better
        return MetricBundle(primary=-avg_loss, metrics={"bce": avg_loss})

    def sample_batch(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        bsz = int(self.cfg["data"]["batch_size"])
        return self._make_batch(bsz)
